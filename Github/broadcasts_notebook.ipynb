{"cells":[{"cell_type":"markdown","metadata":{"cell_id":"407c71a11ead4431bd40d94457c7cff2","deepnote_cell_type":"text-cell-h1","formattedRanges":[]},"source":["# imports"]},{"cell_type":"code","execution_count":null,"metadata":{"cell_id":"5e68b11b78514de7ad7ddfb6e0aa528d","deepnote_cell_type":"code","deepnote_to_be_reexecuted":false,"execution_millis":5565,"execution_start":1694271660208,"source_hash":null},"outputs":[],"source":["import matplotlib.pyplot as plt\n","import pandas as pd\n","import plotly.express as px\n","import plotly.graph_objects as go\n","from plotly.subplots import make_subplots\n","import numpy as np\n","from scipy.cluster.hierarchy import dendrogram, linkage\n","from scipy.spatial.distance import pdist\n","import plotly.graph_objects as go\n","from scipy.spatial.distance import euclidean\n","import seaborn as sns\n","from sklearn.preprocessing import StandardScaler, MinMaxScaler\n","import skfuzzy as fuzz\n","from sklearn.metrics import silhouette_score, davies_bouldin_score\n","from sklearn.cluster import AgglomerativeClustering, DBSCAN\n","from scipy.cluster.hierarchy import dendrogram, linkage\n","from sklearn.decomposition import PCA\n","import hdbscan\n","from sklearn.ensemble import IsolationForest\n","from sklearn.svm import OneClassSVM\n","from sklearn.neighbors import LocalOutlierFactor, NearestNeighbors"]},{"cell_type":"markdown","metadata":{"cell_id":"d8870ea5b2824d4fad6fc35056cfe8b6","deepnote_cell_type":"text-cell-h1","formattedRanges":[]},"source":["# load data"]},{"cell_type":"code","execution_count":null,"metadata":{"cell_id":"2d3e5778467a4c71864f43ecc7c86517","deepnote_cell_type":"code","deepnote_to_be_reexecuted":false,"execution_millis":8063,"execution_start":1694271665793,"source_hash":null},"outputs":[],"source":["df = pd.read_csv(\"report.csv\")\n","\n","# Keep only the desired columns\n","df = df[['date', 'switch_number', 'interface_name', 'broadcast']]\n","df['date'] = pd.to_datetime(df['date'])\n","\n","# Create a combined switch_interface column\n","df['switch_interface'] = df['switch_number'].astype(str) + \":\" + df['interface_name']\n","df = df[['date', 'switch_interface', 'broadcast']]\n","\n","df = df.groupby('switch_interface').filter(lambda x: x['broadcast'].sum() > 0)\n","\n","def is_constant_difference(x):\n","    # Calculate the difference between consecutive broadcasts\n","    diff = x['broadcast'].diff().dropna()\n","    \n","    # Return whether the set of unique differences contains only one item (indicating a constant difference)\n","    return len(diff.unique()) == 1\n","\n","# Group by switch_interface and filter out those with constant differences in broadcasts\n","df = df.groupby('switch_interface').filter(lambda x: not is_constant_difference(x))\n","\n","# Ensure the data is sorted by interface and date\n","df = df.sort_values(by=['switch_interface', 'date'])\n","\n","# Compute the delta for broadcasts\n","df['broadcast_delta'] = df.groupby('switch_interface')['broadcast'].diff().fillna(0)\n","\n","# Identify interfaces with all broadcast_delta values as 0 using the broadcasts_delta_per_week dataframe\n","interfaces_to_drop = df.groupby('switch_interface').filter(lambda x: x['broadcast_delta'].sum() == 0)['switch_interface'].unique()\n","\n","# Remove these interfaces from the main dataframe df\n","df = df[~df['switch_interface'].isin(interfaces_to_drop)]\n","\n","# Drop the first occurrence for each switch_interface\n","df = df.groupby('switch_interface').apply(lambda x: x.tail(len(x) - 1)).reset_index(drop=True)\n","\n","df"]},{"cell_type":"markdown","metadata":{"cell_id":"1f725229a3384b2aa94ced6ad093bc76","deepnote_cell_type":"text-cell-h1","formattedRanges":[]},"source":["# drop interfaces"]},{"cell_type":"markdown","metadata":{"cell_id":"b2de741c17b84aca9a363bf18791c507","deepnote_cell_type":"text-cell-p","formattedRanges":[]},"source":["those interfaces do not have data about everyday for unknown reason"]},{"cell_type":"code","execution_count":null,"metadata":{"cell_id":"b532f05d997b4e1c948cff8b182775df","deepnote_cell_type":"code","deepnote_to_be_reexecuted":false,"execution_millis":1609,"execution_start":1694271673928,"source_hash":null},"outputs":[],"source":["total_days = df['date'].dt.date.nunique()\n","\n","\n","# Define a function to check if the interface has data for every day\n","def has_data_everyday(x):\n","    return x['date'].dt.date.nunique() == total_days\n","\n","# Use groupby and filter to find interfaces that don't have data every day\n","incomplete_interfaces = df.groupby('switch_interface').filter(lambda x: not has_data_everyday(x))\n","\n","# Print unique interfaces from the filtered result\n","print(incomplete_interfaces['switch_interface'].unique())\n","\n","# Get the list of incomplete interfaces\n","incomplete_interface_list = incomplete_interfaces['switch_interface'].unique()\n","\n","# Remove these interfaces from the dataframe\n","df = df[~df['switch_interface'].isin(incomplete_interface_list)]"]},{"cell_type":"markdown","metadata":{},"source":["## add nulls where data is missing"]},{"cell_type":"markdown","metadata":{},"source":["### show amount of time gaps"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["df['time_diff'] = df.groupby('switch_interface')['date'].diff()\n","\n","# Get counts of each unique time difference\n","time_diff_counts = df['time_diff'].value_counts()\n","\n","# Convert to DataFrame and sort by index (which is the time difference in timedelta format)\n","time_diff_table = time_diff_counts.sort_index().reset_index()\n","time_diff_table.columns = ['Time Gap', 'Frequency']\n","\n","# Convert the 'Time Gap' column to MM:SS format\n","time_diff_table['Time Gap (MM:SS)'] = time_diff_table['Time Gap'].apply(lambda x: f'{x.seconds // 60}:{x.seconds % 60:02d}')\n","\n","# Drop the original 'Time Gap' column and reorder the columns for display\n","time_diff_table = time_diff_table[['Time Gap (MM:SS)', 'Frequency']]\n","\n","print(time_diff_table)\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Ensure the data is sorted by interface and date\n","df = df.sort_values(by=['switch_interface', 'date'])\n","\n","# Convert broadcast to int64 to avoid potential overflow\n","df['broadcast'] = df['broadcast'].astype(np.int64)\n","\n","dfs = []  # this will store each interface's dataframe with missing data added\n","\n","for interface in df['switch_interface'].unique():\n","    subset = df[df['switch_interface'] == interface]\n","    \n","    # Create a specific range for each interface, based on its minimum and maximum dates\n","    full_range = pd.date_range(start=subset['date'].min(), end=subset['date'].max(), freq='5T')\n","    \n","    # Set index to date for reindexing\n","    subset = subset.set_index('date').reindex(full_range, method='pad').reset_index().rename(columns={'index': 'date'})\n","    \n","    # Fill switch_interface for missing rows\n","    subset['switch_interface'].fillna(interface, inplace=True)\n","    \n","    dfs.append(subset)\n","\n","# Combine back into a single dataframe\n","df = pd.concat(dfs).sort_values(by=['switch_interface', 'date']).reset_index(drop=True)\n","\n","df['time_gap'] = df.groupby('switch_interface')['date'].diff()\n","\n","df"]},{"cell_type":"markdown","metadata":{"cell_id":"bd02330739bd4162b12ea30ba37c6f71","deepnote_cell_type":"text-cell-h1","formattedRanges":[]},"source":["# data exploration"]},{"cell_type":"code","execution_count":null,"metadata":{"cell_id":"ae646766bd83416ab8316905d6babe78","deepnote_cell_type":"code","deepnote_to_be_reexecuted":false,"execution_millis":14337,"execution_start":1694264680208,"source_hash":null},"outputs":[],"source":["selected_switch_interfaces = df['switch_interface'].unique()\n","\n","subset = df[df['switch_interface'].isin(selected_switch_interfaces)]\n","\n","# Use broadcast_delta instead of broadcast for the y-axis\n","fig = px.line(subset, x='date', y='broadcast_delta', color='switch_interface', title='Broadcast Delta Trend for Selected Switch-Interfaces Over Time')\n","\n","fig.show()"]},{"cell_type":"code","execution_count":null,"metadata":{"cell_id":"4810f102cf044c2b9baf6dd900651513","deepnote_cell_type":"code","deepnote_to_be_reexecuted":false,"execution_millis":2044,"execution_start":1694264695082,"source_hash":null},"outputs":[],"source":["# Get the week number from the date\n","df['week_number'] = df['date'].dt.isocalendar().week\n","\n","# Group by week number, switch number, and interface name to sum up the broadcast deltas\n","broadcasts_delta_per_week = df.groupby(['week_number', 'switch_interface'])['broadcast_delta'].sum().reset_index()\n","\n","# Construct the week label\n","broadcasts_delta_per_week['week_label'] = 'Week ' + broadcasts_delta_per_week['week_number'].astype(str)\n","\n","heatmap_data = broadcasts_delta_per_week.pivot('switch_interface', 'week_label', 'broadcast_delta')\n","sorted_columns = sorted(heatmap_data.columns, key=lambda x: int(x.split()[1]))\n","heatmap_data = heatmap_data[sorted_columns]\n","\n","plt.figure(figsize=(20, 15))\n","sns.heatmap(heatmap_data, cmap='YlGnBu', linewidths=.5, annot=True, fmt=\".0f\")\n","plt.title('Heatmap of Broadcast Delta by Week and Interface')\n","plt.xlabel('Week In 2023')\n","plt.show()\n"]},{"cell_type":"code","execution_count":null,"metadata":{"cell_id":"f800c418e7614bb6adc10a39232bae0f","deepnote_cell_type":"code","deepnote_to_be_reexecuted":false,"execution_millis":9713,"execution_start":1694264696751,"source_hash":null},"outputs":[],"source":["# Create a FacetGrid that displays a grid of line plots for each switch_interface\n","g = sns.FacetGrid(df, col=\"switch_interface\", col_wrap=5, height=3, aspect=1.5)  # col_wrap defines how many plots per row, adjust as needed\n","g.map(plt.plot, \"date\", \"broadcast_delta\", marker=\".\") \n","\n","# Rotating the x-axis labels for clarity and setting titles for each subplot\n","g.set_xticklabels(rotation=45)\n","g.set_titles(\"{col_name}\")\n","\n","plt.tight_layout()\n","plt.show()"]},{"cell_type":"code","execution_count":null,"metadata":{"cell_id":"eab694cc331d4a3c9ddcab63424c773c","deepnote_cell_type":"code","deepnote_to_be_reexecuted":false,"execution_millis":25306,"execution_start":1694264706500,"source_hash":null},"outputs":[],"source":["# Extract day of the week and create a new column\n","df['day_of_week'] = df['date'].dt.day_name()\n","\n","# Order the days for proper plotting\n","days_order = ['Sunday', 'Monday', 'Tuesday', 'Wednesday', 'Thursday', 'Friday', 'Saturday']\n","\n","# Vibrant color palette\n","palette = sns.color_palette(\"viridis\", 7)\n","weekend_palette = [palette[i] if day not in ['Friday', 'Saturday'] else sns.color_palette(\"rocket\")[2] for i, day in enumerate(days_order)]\n","\n","# Create FacetGrid with box plots\n","g = sns.FacetGrid(df, col=\"switch_interface\", col_wrap=3, height=6, sharey=False)\n","g.map(sns.boxplot, 'day_of_week', 'broadcast_delta', order=days_order, palette=weekend_palette)\n","g.map(sns.pointplot, 'day_of_week', 'broadcast_delta', order=days_order, color='red', markers='D', scale=0.7)\n","\n","# Set titles for each facet and other aesthetics\n","g.set_titles(\"{col_name}\")\n","g.set_axis_labels(\"\", \"Broadcasts\")\n","g.set_xticklabels(rotation=30)\n","\n","plt.tight_layout()\n","plt.show()\n"]},{"cell_type":"code","execution_count":null,"metadata":{"cell_id":"4e5829af63f74ff9acd5362a95a0e199","deepnote_cell_type":"code","deepnote_to_be_reexecuted":false,"execution_millis":9938,"execution_start":1694264731839,"source_hash":null},"outputs":[],"source":["# Sort by 'switch_interface' and 'date'\n","df = df.sort_values(by=['switch_interface', 'date'])\n","\n","# Use a merge operation to get the broadcast values from 7 days ago\n","df_lagged = df.copy()\n","df_lagged['date'] = df_lagged['date'] + pd.Timedelta(days=7)\n","\n","# Merge the dataframe with its lagged version\n","merged = pd.merge(df, df_lagged, on=['switch_interface', 'date'], suffixes=('', '_lagged'))\n","\n","# Now, 'broadcast_lagged' will have the broadcast values from 7 days ago\n","g = sns.FacetGrid(merged, col=\"switch_interface\", col_wrap=4, height=4, sharex=False, sharey=False)\n","g = g.map(plt.scatter, \"broadcast_delta_lagged\", \"broadcast_delta\")\n","\n","plt.show()"]},{"cell_type":"code","execution_count":null,"metadata":{"cell_id":"8c6668c948514fa99216c360b593920b","deepnote_cell_type":"code","deepnote_to_be_reexecuted":false,"execution_millis":15697,"execution_start":1694264741240,"source_hash":null},"outputs":[],"source":["df = df.sort_values(by=['switch_interface', 'date'])\n","df['broadcast_delta'] = df.groupby('switch_interface')['broadcast'].diff().fillna(0)\n","\n","# FacetGrid with KDE for each interface\n","g = sns.FacetGrid(df, col=\"switch_interface\", col_wrap=4, height=3, sharex=False, sharey=False)\n","g.map(sns.kdeplot, 'broadcast_delta', shade=True)\n","\n","# Adjusting the subplots for readability\n","g.set_titles(\"{col_name} interface\")\n","g.set_axis_labels(\"Delta Broadcast\", \"Density\")\n","\n","plt.tight_layout()\n","plt.show()\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"cell_id":"c6f915cec58b4682965d8349195dfe4b","deepnote_cell_type":"code","deepnote_to_be_reexecuted":false,"execution_millis":613,"execution_start":1694264756570,"source_hash":null},"outputs":[],"source":["grouped = df.groupby('switch_interface')['broadcast_delta']\n","\n","stats_df = pd.DataFrame({\n","    'Mean': grouped.mean(),\n","    'Median': grouped.median(),\n","    'Min': grouped.min(),\n","    'Max': grouped.max(),\n","    'Mode': grouped.apply(lambda x: x.mode().iloc[0]),\n","    'Skewness': grouped.skew(),\n","    'Standard Deviation': grouped.std(),\n","    'Kurtosis': grouped.apply(pd.Series.kurt),\n","    '25th Percentile': grouped.quantile(0.25),\n","    '75th Percentile': grouped.quantile(0.75)\n","})\n","\n","styled = stats_df.style.background_gradient(cmap='viridis').set_precision(2)\n","display(styled)\n"]},{"cell_type":"markdown","metadata":{"cell_id":"0298a7990e2d4c43aa378454c13d4ddb","deepnote_cell_type":"text-cell-h1","formattedRanges":[]},"source":["# Modeling"]},{"cell_type":"markdown","metadata":{"cell_id":"32f14497e6584cb29a4aa3ac7450c792","deepnote_cell_type":"text-cell-h2","formattedRanges":[]},"source":["## normalization"]},{"cell_type":"code","execution_count":null,"metadata":{"cell_id":"30429c73caa143adb99d3a43a9975e72","deepnote_cell_type":"code","deepnote_to_be_reexecuted":false,"execution_millis":62,"execution_start":1694271675488,"source_hash":null},"outputs":[],"source":["# Normalize delta_broadcast\n","scaler = MinMaxScaler()  # or StandardScaler() for Z-score normalization\n","df['normalized_broadcast_delta'] = scaler.fit_transform(df[['broadcast_delta']])"]},{"cell_type":"markdown","metadata":{"cell_id":"8c4a5c6c14e346c18973547e0184514b","deepnote_cell_type":"text-cell-h3","formattedRanges":[]},"source":["### fuzzy-Kmeans"]},{"cell_type":"markdown","metadata":{"cell_id":"4d59f2217363460b8b4bea02423f536c","deepnote_cell_type":"text-cell-p","formattedRanges":[]},"source":["model and scores"]},{"cell_type":"code","execution_count":null,"metadata":{"cell_id":"f72d987ad1c24dbe9faa309f6c7b19a2","deepnote_cell_type":"code","deepnote_to_be_reexecuted":false,"execution_millis":1470425,"execution_start":1694264757052,"source_hash":null},"outputs":[],"source":["# List of unique interfaces\n","interfaces = df['switch_interface'].unique()\n","\n","# This will store the best parameters for each interface\n","interface_params = {}\n","interface_metrics = {}\n","\n","# Define range of clusters and fuzziness coefficients to test\n","clusters_range = range(2, 3)  # starting from 2 because 1 cluster is trivial\n","m_values = np.linspace(1.5, 3, 6)  # dividing the range [1.5,3] into 6 values\n","\n","\n","# 1. Determine best parameters for each interface\n","for interface in interfaces:\n","    interface_data = df[df['switch_interface'] == interface]\n","    data_to_cluster = interface_data['normalized_broadcast_delta'].values.reshape(-1, 1).T\n","\n","    best_obj_func = float('inf')\n","    best_params = {}\n","\n","    for n_clusters in clusters_range:\n","        for m in m_values:\n","            _, _, _, _, jm, _, _ = fuzz.cluster.cmeans(data_to_cluster, c=n_clusters, m=m, error=0.005, maxiter=1000)\n","            if jm[-1] < best_obj_func:\n","                best_obj_func = jm[-1]\n","                best_params = {'clusters': n_clusters, 'm': m}\n","\n","    interface_params[interface] = best_params\n","\n","# 2. Compute silhouette and Davies-Bouldin scores using best parameters\n","for interface in interfaces:\n","    interface_data = df[df['switch_interface'] == interface]\n","    data_to_cluster = interface_data['normalized_broadcast_delta'].values.reshape(-1, 1)\n","    \n","    best_params = interface_params[interface]\n","    _, u, _, _, _, _, _ = fuzz.cluster.cmeans(data_to_cluster.T, c=best_params['clusters'], m=best_params['m'], error=0.005, maxiter=1000)\n","    \n","    labels = np.argmax(u, axis=0)\n","    labels = labels.flatten()\n","    silhouette = silhouette_score(data_to_cluster, labels)\n","    davies_bouldin = davies_bouldin_score(data_to_cluster, labels)\n","\n","    interface_metrics[interface] = {\n","        'best_params': best_params,\n","        'silhouette_score': silhouette,\n","        'davies_bouldin_score': davies_bouldin\n","    }\n","\n","\n","# Compute the mean of the scores across all interfaces\n","total_silhouette = 0\n","total_davies_bouldin = 0\n","num_interfaces = len(interface_metrics)\n","\n","for metrics in interface_metrics.values():\n","    total_silhouette += metrics['silhouette_score']\n","    total_davies_bouldin += metrics['davies_bouldin_score']\n","\n","mean_silhouette = total_silhouette / num_interfaces\n","mean_davies_bouldin = total_davies_bouldin / num_interfaces\n","\n","for interface, metrics in interface_metrics.items():\n","    print(f\"For interface {interface}, best parameters are: {metrics['best_params']}\")\n","    print(f\"Silhouette Score: {metrics['silhouette_score']:.4f}\")\n","    print(f\"Davies-Bouldin Index: {metrics['davies_bouldin_score']:.4f}\")\n","    print('-'*50)\n","\n","# Print the mean scores\n","print(\"Mean Silhouette Score across all interfaces:\", mean_silhouette)\n","print(\"Mean Davies-Bouldin Score across all interfaces:\", mean_davies_bouldin)\n"]},{"cell_type":"markdown","metadata":{"cell_id":"76cc095e052d41049505ca27eef92569","deepnote_cell_type":"text-cell-p","formattedRanges":[]},"source":["evaluation"]},{"cell_type":"code","execution_count":null,"metadata":{"cell_id":"cb6fbf2cd8d04b67a901e959bf2d2378","deepnote_cell_type":"code","deepnote_to_be_reexecuted":false,"execution_millis":1795,"execution_start":1694270507923,"source_hash":null},"outputs":[],"source":["# Extracting interface names and their corresponding scores for plotting\n","interface_names = list(interface_metrics.keys())\n","silhouette_scores = [metrics['silhouette_score'] for metrics in interface_metrics.values()]\n","davies_bouldin_scores = [metrics['davies_bouldin_score'] for metrics in interface_metrics.values()]\n","\n","# Sort the interface names and scores based on the size of the column (broadcast_delta)\n","sorted_indices = np.argsort(silhouette_scores)  # You can use silhouette_scores or davies_bouldin_scores\n","interface_names = [interface_names[i] for i in sorted_indices]\n","silhouette_scores = [silhouette_scores[i] for i in sorted_indices]\n","davies_bouldin_scores = [davies_bouldin_scores[i] for i in sorted_indices]\n","\n","# Plotting Silhouette Scores\n","plt.figure(figsize=(15,7))\n","plt.bar(interface_names, silhouette_scores, color='blue')\n","plt.xlabel('Interface')\n","plt.ylabel('Silhouette Score')\n","plt.title('Silhouette Score per Interface')\n","plt.xticks(rotation=90)\n","plt.tight_layout()\n","plt.show()\n","\n","# Plotting Davies-Bouldin Scores\n","plt.figure(figsize=(15,7))\n","plt.bar(interface_names, davies_bouldin_scores, color='red')\n","plt.xlabel('Interface')\n","plt.ylabel('Davies-Bouldin Index')\n","plt.title('Davies-Bouldin Index per Interface')\n","plt.xticks(rotation=90)\n","plt.tight_layout()\n","plt.show()"]},{"cell_type":"code","execution_count":null,"metadata":{"cell_id":"931170ab2e934028887786b021bca2c4","deepnote_cell_type":"code","deepnote_to_be_reexecuted":false,"execution_millis":138051,"execution_start":1694266228569,"source_hash":null},"outputs":[],"source":["df['early_kmeans_cluster_label'] = -1  # Initialize to -1\n","\n","# 3. Assign cluster labels to each point using best parameters\n","for interface in interfaces:\n","    interface_data = df[df['switch_interface'] == interface]\n","    data_to_cluster = interface_data['broadcast_delta'].values.reshape(-1, 1)\n","\n","    # Check if the interface exists in the dictionary\n","    if interface in interface_params:\n","        best_params = interface_params[interface]\n","        _, u, _, _, _, _, _ = fuzz.cluster.cmeans(data_to_cluster.T, c=best_params['clusters'], m=best_params['m'], error=0.005, maxiter=1000)\n","\n","        labels = np.argmax(u, axis=0)\n","        labels = labels.flatten()\n","\n","        # Assign cluster labels to the corresponding data points in the original DataFrame\n","        df.loc[df['switch_interface'] == interface, 'early_kmeans_cluster_label'] = labels\n","\n","# Create a custom color palette to ensure unique colors for each cluster\n","num_clusters = len(df['early_kmeans_cluster_label'].unique())\n","custom_palette = sns.color_palette(\"Set1\", num_clusters)\n","\n","\n","# Create a larger facet grid with individual plots for each interface\n","g = sns.FacetGrid(df, col=\"switch_interface\", col_wrap=3, height=5, sharey=False)\n","g.map_dataframe(sns.scatterplot, x=\"date\", y=\"broadcast_delta\", hue=\"early_kmeans_cluster_label\", palette=\"Set1\", alpha=0.5)\n","g.add_legend(title=\"Cluster\")\n","g.set_axis_labels(\"Date\", \"Broadcast Delta\")\n","g.set_titles(\"Interface {col_name}\")\n","\n","# Change legend labels to 'Cluster A', 'Cluster B', 'Cluster C', etc.\n","new_legend_labels = [f\"Cluster {chr(65 + i)}\" for i in range(len(df['early_kmeans_cluster_label'].unique()))]\n","g._legend.set_title(\"Cluster\")\n","for t, l in zip(g._legend.texts, new_legend_labels):\n","    t.set_text(l)\n","\n","# Show the facet grid plots\n","plt.show()"]},{"cell_type":"markdown","metadata":{"cell_id":"f3f0d1dc49c64e48b1284d794a743662","deepnote_cell_type":"text-cell-h3","formattedRanges":[]},"source":["### HDBSCAN"]},{"cell_type":"code","execution_count":null,"metadata":{"cell_id":"4d32a2156d5a43808240ad7e3d26ebfa","deepnote_cell_type":"code","deepnote_table_loading":false,"deepnote_table_state":{"filters":[],"pageIndex":0,"pageSize":10,"sortBy":[{"id":"broadcast_delta","type":"desc"}]},"deepnote_to_be_reexecuted":false,"execution_millis":3078,"execution_start":1694271742388,"source_hash":null},"outputs":[],"source":["# 1. Preprocess the data\n","scaled_data = StandardScaler().fit_transform(df['broadcast_delta'].values.reshape(-1, 1))\n","\n","# 2. Apply HDBSCAN clustering\n","# Adjust min_cluster_size to a larger value to reduce the number of small clusters.\n","clusterer = hdbscan.HDBSCAN(min_samples=100, min_cluster_size=100000, gen_min_span_tree=True)\n","cluster_labels = clusterer.fit_predict(scaled_data)\n","df['hdbscan_cluster'] = cluster_labels\n","\n","# 3. Visualization\n","# Histogram of cluster assignments\n","plt.figure(figsize=(10, 5))\n","sns.countplot(cluster_labels)\n","plt.title('Cluster Sizes with HDBSCAN')\n","plt.xlabel('Cluster Label')\n","plt.ylabel('Number of Points')\n","plt.show()\n","\n","# Scatter plot for each interface's broadcast delta with cluster labels\n","g = sns.FacetGrid(df, col=\"switch_interface\", col_wrap=3, height=5, sharey=False, hue='hdbscan_cluster', palette='Spectral')\n","g.map(plt.scatter, \"date\", \"broadcast_delta\", alpha=0.5).add_legend()\n","g.set_axis_labels(\"Time\", \"Broadcast Delta\")\n","g.set_titles(\"Interface {col_name}\")\n","plt.show()\n"]},{"cell_type":"markdown","metadata":{},"source":["### isolation tree"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Custom hyperparameters for specific interfaces\n","custom_hyperparameters = {\n","    'FastEthernet14': {'contamination': 0.15},\n","    'FastEthernet30': {'contamination': 0.15},\n","    'FastEthernet43': {'contamination': 0.15},\n","    'GigabitEthernet1': {'contamination': 0.15},\n","    'GigabitEthernet2': {'contamination': 0.15},\n","    'GigabitEthernet36': {'contamination': 0.03},\n","    'GigabitEthernet37': {'contamination': 0.03},\n","    'GigabitEthernet39': {'contamination': 0.03},\n","    'GigabitEthernet1/1/4': {'contamination': 0.03},\n","}\n","\n","anomalies_df = pd.DataFrame()\n","\n","# Loop through each unique interface\n","for interface in df['switch_interface'].unique():\n","    \n","    # Extract data corresponding to the current interface\n","    subset = df[df['switch_interface'] == interface].copy()\n","    \n","    # Set default hyperparameters\n","    isolation_forest_params = {\n","        'contamination': 0.05  # default value\n","    }\n","    \n","    # Check if there are custom hyperparameters for this interface\n","    if interface in custom_hyperparameters:\n","        isolation_forest_params.update(custom_hyperparameters[interface])\n","    \n","    # Define the Isolation Forest with parameters\n","    clf = IsolationForest(**isolation_forest_params)\n","    \n","    # Apply Isolation Forest\n","    df.loc[subset.index, 'early_ist_anomaly'] = clf.fit_predict(subset[['broadcast_delta']])\n","    subset['early_ist_anomaly'] = df.loc[subset.index, 'early_ist_anomaly']\n","\n","    \n","    # Append anomalies to the main DataFrame\n","    anomalies_df = anomalies_df.append(subset[subset['early_ist_anomaly'] == -1])\n","\n","# Plotting the results using FacetGrid\n","g = sns.FacetGrid(df, col=\"switch_interface\", col_wrap=4, height=4, sharey=False)\n","g.map_dataframe(sns.lineplot, x=\"date\", y=\"broadcast_delta\", hue=\"early_ist_anomaly\", palette={-1: \"r\", 1: \"b\"})\n","g.set_axis_labels(\"Time\", \"Broadcast Delta\")\n","g.set_titles(col_template=\"{col_name} interface\")\n","g.set(xticks=[])\n","plt.show()"]},{"cell_type":"markdown","metadata":{},"source":["## hybrid with Threshold-based Anomaly Detection"]},{"cell_type":"markdown","metadata":{},"source":["### detecting anomalies on daily basis"]},{"cell_type":"markdown","metadata":{},"source":["calculate days that contains anomalies"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# List of unique interfaces\n","unique_interfaces = df['switch_interface'].unique()\n","\n","window_size = 12\n","multiplier = 3\n","\n","# Add a new column 'date_only' which contains only the date without the time component\n","df['date_only'] = df['date'].dt.date\n","\n","# Initialize a new anomaly column in the original df with False values\n","df['day_anomaly'] = False\n","df['daily_rolling_mean'] = None\n","df['daily_rolling_std'] = None\n","\n","for interface in unique_interfaces:\n","    subset_df = df[df['switch_interface'] == interface].copy()\n","\n","    # Aggregate on a daily basis\n","    subset_daily = subset_df.set_index('date').resample('D').sum().reset_index()\n","    \n","    # Compute rolling metrics\n","    subset_daily['daily_rolling_mean'] = subset_daily['broadcast_delta'].rolling(window=window_size).mean()\n","    subset_daily['daily_rolling_std'] = subset_daily['broadcast_delta'].rolling(window=window_size).std()\n","    \n","    # Detect anomalies\n","    subset_daily['anomaly'] = subset_daily['broadcast_delta'] > (subset_daily['daily_rolling_mean'] + multiplier * subset_daily['daily_rolling_std'])\n","    \n","    # Map the rolling metrics and anomaly flags back to the original dataframe\n","    df.loc[df['switch_interface'] == interface, 'day_anomaly'] = df['date_only'].map(subset_daily.set_index('date')['anomaly'])\n","    df.loc[df['switch_interface'] == interface, 'daily_rolling_mean'] = df['date_only'].map(subset_daily.set_index('date')['daily_rolling_mean'])\n","    df.loc[df['switch_interface'] == interface, 'daily_rolling_std'] = df['date_only'].map(subset_daily.set_index('date')['daily_rolling_std'])\n"]},{"cell_type":"markdown","metadata":{},"source":["plot interfaces in which a daily anomaly was detedted"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Plotting\n","for interface in unique_interfaces:\n","    subset_df = df[df['switch_interface'] == interface].copy()\n","\n","    # Skip plotting if no anomalies detected for this interface\n","    if not subset_df['day_anomaly'].any():\n","        continue\n","\n","    # Create traces for broadcast delta\n","    trace0 = go.Scatter(\n","        x=subset_df['date'],\n","        y=subset_df['broadcast_delta'],\n","        mode='lines',\n","        name='Broadcast Delta Daily'\n","    )\n","\n","    # Get anomaly windows for shaded regions\n","    anomaly_windows = []\n","    in_anomaly_window = False\n","    window_start = None\n","    SHIFT_PERIODS = window_size // 1000  # Using the window size here, adjust based on observation\n","    previous_window_end = None\n","\n","    for idx, row in subset_df.iterrows():\n","        if row['day_anomaly'] and not in_anomaly_window:\n","            in_anomaly_window = True\n","            window_start = row['date'] - pd.Timedelta(days=SHIFT_PERIODS)\n","            \n","            # Ensure no overlap with previous window\n","            if previous_window_end and window_start <= previous_window_end:\n","                window_start = previous_window_end + pd.Timedelta(days=1)\n","\n","        elif not row['day_anomaly'] and in_anomaly_window:\n","            in_anomaly_window = False\n","            window_end = row['date'] + pd.Timedelta(days=1)\n","            anomaly_windows.append((window_start, window_end))\n","            previous_window_end = window_end\n","\n","    # In case data ends within an anomaly window\n","    if in_anomaly_window:\n","        anomaly_windows.append((window_start, subset_df['date'].iloc[-1] + pd.Timedelta(days=1)))\n","\n","    shapes = []\n","    for start, end in anomaly_windows:\n","        shapes.append({\n","            'type': 'rect',\n","            'x0': start,\n","            'x1': end,\n","            'y0': 0,\n","            'y1': subset_df['broadcast_delta'].max(),\n","            'fillcolor': 'red',\n","            'opacity': 0.4,\n","            'line_width': 0,\n","        })\n","\n","    layout = go.Layout(\n","        title=f'Daily Broadcast Delta with Anomalies for Interface {interface}',\n","        xaxis=dict(title='Date'),\n","        yaxis=dict(title='Broadcast Delta Daily'),\n","        shapes=shapes\n","    )\n","\n","    fig = go.Figure(data=[trace0], layout=layout)\n","    fig.show()\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["unique_interfaces = df['switch_interface'].unique()\n","\n","window_size_5min = 24  # This corresponds to 2 hours of data if each record is 5 minutes apart\n","multiplier_5min = 2\n","\n","anomalies_data_5min = {}\n","\n","unique_interfaces = df['switch_interface'].unique()\n","\n","window_size_5min = 24  # This corresponds to 2 hours of data if each record is 5 minutes apart\n","multiplier_5min = 2\n","\n","anomalies_data_5min = {}\n","\n","for interface in unique_interfaces:\n","    subset_df = df[df['switch_interface'] == interface].copy()\n","    \n","    # Compute rolling metrics on the 5-min data\n","    subset_df['rolling_mean_5min'] = subset_df['broadcast_delta'].rolling(window=window_size_5min).mean()\n","    subset_df['rolling_std_5min'] = subset_df['broadcast_delta'].rolling(window=window_size_5min).std()\n","    \n","    # Detect anomalies on the 5-min data\n","    subset_df['anomaly_5min'] = subset_df['broadcast_delta'] > (subset_df['rolling_mean_5min'] + multiplier_5min * subset_df['rolling_std_5min'])\n","    \n","    # Store the data in the anomalies dictionary\n","    anomalies_data_5min[interface] = subset_df\n","\n","# Initialize columns in the original df \n","df['anomaly_5min'] = False\n","df['rolling_mean_5min'] = np.nan\n","df['rolling_std_5min'] = np.nan\n","\n","# Update the 'anomaly_5min', 'rolling_mean_5min', and 'rolling_std_5min' columns in the original df\n","for interface, subset_df in anomalies_data_5min.items():\n","    idx = df['switch_interface'] == interface\n","    df.loc[idx, 'anomaly_5min'] = subset_df['anomaly_5min'].values\n","    df.loc[idx, 'rolling_mean_5min'] = subset_df['rolling_mean_5min'].values\n","    df.loc[idx, 'rolling_std_5min'] = subset_df['rolling_std_5min'].values\n","\n","\n","# Plotting\n","for interface, subset_df in anomalies_data_5min.items():\n","    \n","    # Skip plotting if no anomalies detected for this interface\n","    if not subset_df['anomaly_5min'].any():\n","        continue\n","\n","    # Create traces for broadcast delta\n","    trace0 = go.Scatter(\n","        x = subset_df['date'],\n","        y = subset_df['broadcast_delta'],\n","        mode = 'lines',\n","        name = 'Broadcast Delta'\n","    )\n","    \n","    # Get anomaly windows for shaded regions\n","    anomaly_windows = []\n","    in_anomaly_window = False\n","    window_start = None\n","\n","    # Constant to shift the anomaly window to better capture the start of the spike\n","    SHIFT_PERIODS = window_size_5min // 10\n","\n","    previous_window_end = None\n","\n","    for idx, row in subset_df.iterrows():\n","        if row['anomaly_5min'] and not in_anomaly_window:\n","            in_anomaly_window = True\n","            window_start = row['date'] - pd.Timedelta(minutes=5*SHIFT_PERIODS)\n","            \n","            # Ensure no overlap with previous window\n","            if previous_window_end and window_start <= previous_window_end:\n","                window_start = previous_window_end + pd.Timedelta(minutes=5)\n","\n","        elif not row['anomaly_5min'] and in_anomaly_window:\n","            in_anomaly_window = False\n","            window_end = row['date'] + pd.Timedelta(minutes=5)\n","            anomaly_windows.append((window_start, window_end))\n","            previous_window_end = window_end\n","\n","    # In case data ends within an anomaly window\n","    if in_anomaly_window:\n","        anomaly_windows.append((window_start, subset_df['date'].iloc[-1] + pd.Timedelta(minutes=5)))\n","\n","    shapes = []\n","    for start, end in anomaly_windows:\n","        shapes.append({\n","            'type': 'rect',\n","            'x0': start,\n","            'x1': end,\n","            'y0': 0,\n","            'y1': subset_df['broadcast_delta'].max(),\n","            'fillcolor': 'red',\n","            'opacity': 0.4,\n","            'line_width': 0,\n","        })\n","    \n","    layout = go.Layout(\n","        title=f'Broadcast Delta with 5-min Anomalies for Interface {interface}',\n","        xaxis=dict(title='Date'),\n","        yaxis=dict(title='Broadcast Delta'),\n","        shapes=shapes\n","    )\n","    \n","    fig = go.Figure(data=[trace0], layout=layout)\n","    fig.show()\n","\n","\n","# Initialize a new 5-minute anomaly column in the original df with False values\n","df['anomaly_5min'] = False\n","\n","# Update the 'anomaly_5min' column in the original df based on the detected anomalies\n","for interface, subset_df in anomalies_data_5min.items():\n","    # Create a dictionary with date as key and anomaly flag as value for the current subset\n","    anomaly_dict_5min = dict(zip(subset_df['date'], subset_df['anomaly_5min']))\n","    \n","    # Update the 'anomaly_5min' column in the original df\n","    df.loc[df['switch_interface'] == interface, 'anomaly_5min'] = df['date'].map(anomaly_dict_5min)"]},{"cell_type":"markdown","metadata":{},"source":["### add anomalies by the hour"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["unique_interfaces = df['switch_interface'].unique()\n","\n","# Adjusting for hourly window. If each record is 5 minutes apart, 12 records would make up an hour.\n","window_size_hour = 12\n","multiplier_hour = 2\n","\n","# Iterate through each unique interface\n","for interface in unique_interfaces:\n","    # Get the indices of rows corresponding to the current interface\n","    subset_indices = df[df['switch_interface'] == interface].index\n","    \n","    # Compute rolling metrics for the current interface\n","    df.loc[subset_indices, 'rolling_mean_hour'] = df.loc[subset_indices, 'broadcast_delta'].rolling(window=window_size_hour).mean()\n","    df.loc[subset_indices, 'rolling_std_hour'] = df.loc[subset_indices, 'broadcast_delta'].rolling(window=window_size_hour).std()\n","\n","    # Detect and mark anomalies for the current interface based on the rolling metrics\n","    df.loc[subset_indices, 'anomaly_hour'] = df.loc[subset_indices, 'broadcast_delta'] > (df.loc[subset_indices, 'rolling_mean_hour'] + multiplier_hour * df.loc[subset_indices, 'rolling_std_hour'])\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Plotting\n","for interface in unique_interfaces:\n","    subset_df = df[df['switch_interface'] == interface].copy()\n","\n","    # Skip plotting if no anomalies detected for this interface\n","    if not subset_df['anomaly_hour'].any():\n","        continue\n","\n","    # Create traces for broadcast delta\n","    trace0 = go.Scatter(\n","        x=subset_df['date'],\n","        y=subset_df['broadcast_delta'],\n","        mode='lines',\n","        name='Broadcast Delta Hourly'\n","    )\n","\n","    # Get anomaly windows for shaded regions\n","    anomaly_windows = []\n","    in_anomaly_window = False\n","    window_start = None\n","    \n","    # Using one-tenth the window size as shift. Adjust based on observation.\n","    SHIFT_PERIODS = window_size_hour // 10  \n","    previous_window_end = None\n","\n","    for idx, row in subset_df.iterrows():\n","        if row['anomaly_hour'] and not in_anomaly_window:\n","            in_anomaly_window = True\n","            window_start = row['date'] - pd.Timedelta(hours=SHIFT_PERIODS)\n","            \n","            # Ensure no overlap with previous window\n","            if previous_window_end and window_start <= previous_window_end:\n","                window_start = previous_window_end + pd.Timedelta(hours=1)\n","\n","        elif not row['anomaly_hour'] and in_anomaly_window:\n","            in_anomaly_window = False\n","            window_end = row['date'] + pd.Timedelta(hours=1)\n","            anomaly_windows.append((window_start, window_end))\n","            previous_window_end = window_end\n","\n","    # In case data ends within an anomaly window\n","    if in_anomaly_window:\n","        anomaly_windows.append((window_start, subset_df['date'].iloc[-1] + pd.Timedelta(hours=1)))\n","\n","    shapes = []\n","    for start, end in anomaly_windows:\n","        shapes.append({\n","            'type': 'rect',\n","            'x0': start,\n","            'x1': end,\n","            'y0': 0,\n","            'y1': subset_df['broadcast_delta'].max(),\n","            'fillcolor': 'red',\n","            'opacity': 0.4,\n","            'line_width': 0,\n","        })\n","\n","    layout = go.Layout(\n","        title=f'Hourly Broadcast Delta with Anomalies for Interface {interface}',\n","        xaxis=dict(title='Date'),\n","        yaxis=dict(title='Broadcast Delta Hourly'),\n","        shapes=shapes\n","    )\n","\n","    fig = go.Figure(data=[trace0], layout=layout)\n","    fig.show()\n"]},{"cell_type":"markdown","metadata":{},"source":["### IsolationForest"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["unique_interfaces = df['switch_interface'].unique()\n","\n","# Hyperparameters for the Isolation Forest\n","n_estimators = 100\n","max_samples = 'auto'\n","contamination = 0.05  # proportion of outliers in the data set, adjust based on your domain knowledge\n","\n","# Create columns for anomaly scores and flags\n","df['anomaly_iforest_score'] = 0\n","df['anomaly_iforest'] = 0\n","\n","for interface in unique_interfaces:\n","    subset_df = df[df['switch_interface'] == interface].copy()\n","    \n","    # Extracting features\n","    features = subset_df[['normalized_broadcast_delta', 'day_anomaly', 'anomaly_5min', 'anomaly_hour']]\n","    \n","    # Since 'day_anomaly' and '5min_anomaly' are boolean, convert them to integer type\n","    features['day_anomaly'] = features['day_anomaly'].astype(int)\n","    features['anomaly_5min'] = features['anomaly_5min'].astype(int)\n","    features['anomaly_hour'] = features['anomaly_hour'].astype(int)\n","\n","    \n","    # Normalize the data\n","    scaler = StandardScaler()\n","    features_normalized = scaler.fit_transform(features)\n","    \n","    # Train the Isolation Forest\n","    clf = IsolationForest(n_estimators=n_estimators, max_samples=max_samples, contamination=contamination, random_state=42)\n","    clf.fit(features_normalized)\n","    \n","    # Predict anomalies and store them in the main dataframe\n","    df.loc[df['switch_interface'] == interface, 'anomaly_iforest_score'] = clf.decision_function(features_normalized)\n","    df.loc[df['switch_interface'] == interface, 'anomaly_iforest'] = clf.predict(features_normalized)\n","\n","# Convert the anomaly flags: -1 means anomaly and 1 means normal data in Isolation Forest\n","df['anomaly_iforest'] = df['anomaly_iforest'].apply(lambda x: True if x == -1 else False)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["for interface in unique_interfaces:\n","    subset_df = df[df['switch_interface'] == interface].copy()\n","    \n","    # Create traces for broadcast delta\n","    trace0 = go.Scatter(\n","        x = subset_df['date'],\n","        y = subset_df['broadcast_delta'],\n","        mode = 'lines',\n","        name = 'Broadcast Delta'\n","    )\n","    \n","    # Create traces for anomalies detected by Isolation Forest\n","    trace1 = go.Scatter(\n","        x = subset_df[subset_df['anomaly_iforest']]['date'],\n","        y = subset_df[subset_df['anomaly_iforest']]['broadcast_delta'],\n","        mode = 'markers',\n","        name = 'Anomalies by Isolation Forest',\n","        marker=dict(color='red', size=8)\n","    )\n","    \n","    layout = go.Layout(\n","        title=f'Broadcast Delta with Anomalies by Isolation Forest for Interface {interface}',\n","        xaxis=dict(title='Date'),\n","        yaxis=dict(title='Broadcast Delta'),\n","    )\n","    \n","    fig = go.Figure(data=[trace0, trace1], layout=layout)\n","    fig.show()"]},{"cell_type":"markdown","metadata":{},"source":["### One-class SVM"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["\n","# Parameter for One-Class SVM. You might need to adjust based on performance.\n","nu_val = 0.05  # An upper bound on the fraction of margin errors and a lower bound of the fraction of support vectors\n","\n","df['anomaly_ocsvm'] = 0  # Initialize with 0\n","\n","\n","for interface in unique_interfaces:\n","    subset_df = df[df['switch_interface'] == interface].copy()\n","    \n","    # Prepare the data: normalized_broadcast_delta and anomaly flags\n","    X = subset_df[['normalized_broadcast_delta', 'day_anomaly', 'anomaly_5min']]\n","    \n","    # Train One-Class SVM\n","    ocsvm = OneClassSVM(nu=nu_val, kernel=\"rbf\", gamma='auto')\n","    ocsvm.fit(X)\n","    \n","    # Get predictions\n","    preds = ocsvm.predict(X)\n","    \n","    # Convert predictions to a boolean anomaly flag (True if anomaly, else False)\n","    subset_df['anomaly_ocsvm'] = preds == -1\n","\n","    # Update the original DataFrame\n","    df.loc[subset_df.index, 'anomaly_ocsvm'] = subset_df['anomaly_ocsvm']\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Now, plotting the results\n","\n","for interface in unique_interfaces:\n","    subset_df = df[df['switch_interface'] == interface].copy()\n","    \n","    # Create traces for broadcast delta\n","    trace0 = go.Scatter(\n","        x = subset_df['date'],\n","        y = subset_df['broadcast_delta'],\n","        mode = 'lines',\n","        name = 'Broadcast Delta'\n","    )\n","    \n","    # Create traces for anomalies detected by One-Class SVM\n","    trace1 = go.Scatter(\n","        x = subset_df[subset_df['anomaly_ocsvm']]['date'],\n","        y = subset_df[subset_df['anomaly_ocsvm']]['broadcast_delta'],\n","        mode = 'markers',\n","        name = 'Anomalies by One-Class SVM',\n","        marker=dict(color='red', size=8)\n","    )\n","    \n","    layout = go.Layout(\n","        title=f'Broadcast Delta with Anomalies by One-Class SVM for Interface {interface}',\n","        xaxis=dict(title='Date'),\n","        yaxis=dict(title='Broadcast Delta'),\n","    )\n","    \n","    fig = go.Figure(data=[trace0, trace1], layout=layout)\n","    fig.show()\n"]},{"cell_type":"markdown","metadata":{},"source":["### LOF"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["df['anomaly_lof'] = False\n","\n","\n","unique_interfaces = df['switch_interface'].unique()\n","\n","for interface in unique_interfaces:\n","    \n","    subset_df = df[df['switch_interface'] == interface].copy()\n","\n","    # Extract features\n","    X = subset_df[['normalized_broadcast_delta', 'day_anomaly', 'anomaly_5min', 'anomaly_hour']]\n","\n","    # Train LOF Model\n","    lof = LocalOutlierFactor(n_neighbors=20, contamination=0.05)\n","    lof_predictions = lof.fit_predict(X)\n","    \n","    # Convert LOF predictions (-1 for outliers, 1 for inliers) to boolean (True for outliers, False for inliers)\n","    subset_df['anomaly_lof'] = lof_predictions == -1\n","\n","    # Update the main DataFrame with LOF results for the current interface\n","    df.loc[df['switch_interface'] == interface, 'anomaly_lof'] = subset_df['anomaly_lof']\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["for interface in unique_interfaces:\n","    \n","    subset_df = df[df['switch_interface'] == interface].copy()\n","\n","    # Plot data\n","    trace1 = go.Scatter(\n","        x=subset_df['date'],\n","        y=subset_df['broadcast_delta'],  # Using the original broadcast_delta for visualization\n","        mode='lines',\n","        name='Data'\n","    )\n","\n","    # Highlight anomalies with a different color\n","    anomalies = subset_df[subset_df['anomaly_lof'] == True]\n","    \n","    if not anomalies.empty:  # Only plot if there are anomalies\n","        trace2 = go.Scatter(\n","            x=anomalies['date'],\n","            y=anomalies['broadcast_delta'],\n","            mode='markers',\n","            name='Anomaly',\n","            marker=dict(color='red', size=10)\n","        )\n","\n","        layout = go.Layout(\n","            title=f'Broadcast Delta with LOF Anomalies for Interface {interface}',\n","            xaxis=dict(title='Date'),\n","            yaxis=dict(title='Broadcast Delta')\n","        )\n","\n","        fig = go.Figure(data=[trace1, trace2], layout=layout)\n","        fig.show()\n"]},{"cell_type":"markdown","metadata":{},"source":["### fuzzy kmeans with tagged anomalies"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# List of unique interfaces\n","interfaces = df['switch_interface'].unique()\n","\n","# This will store the best parameters for each interface\n","interface_params = {}\n","interface_metrics = {}\n","\n","# Define range of clusters and fuzziness coefficients to test\n","clusters_range = range(2, 3)  # starting from 2 because 1 cluster is trivial\n","m_values = np.linspace(1.5, 3, 6)  # dividing the range [1.5,3] into 6 values\n","\n","# 1. Determine best parameters for each interface\n","for interface in interfaces:\n","    interface_data = df[df['switch_interface'] == interface]\n","    data_to_cluster = interface_data[['normalized_broadcast_delta', 'day_anomaly', 'anomaly_hour', 'rolling_mean_5min', 'rolling_std_5min']].values.T\n","\n","    best_obj_func = float('inf')\n","    best_params = {}\n","\n","    for n_clusters in clusters_range:\n","        for m in m_values:\n","            _, _, _, _, jm, _, _ = fuzz.cluster.cmeans(data_to_cluster, c=n_clusters, m=m, error=0.005, maxiter=1000)\n","            if jm[-1] < best_obj_func:\n","                best_obj_func = jm[-1]\n","                best_params = {'clusters': n_clusters, 'm': m}\n","\n","    interface_params[interface] = best_params\n","\n","# 2. Compute silhouette and Davies-Bouldin scores using best parameters\n","for interface in interfaces:\n","    interface_data = df[df['switch_interface'] == interface]\n","    data_to_cluster_df = interface_data[['normalized_broadcast_delta', 'day_anomaly','anomaly_hour', 'rolling_mean_5min', 'rolling_std_5min']]\n","    \n","    # Remove rows with NaN values\n","    data_to_cluster_df = data_to_cluster_df.dropna()\n","    data_to_cluster = data_to_cluster_df.values\n","\n","    best_params = interface_params[interface]\n","    _, u, _, _, _, _, _ = fuzz.cluster.cmeans(data_to_cluster.T, c=best_params['clusters'], m=best_params['m'], error=0.005, maxiter=1000)\n","    \n","    labels = np.argmax(u, axis=0)\n","    labels = labels.flatten()\n","    silhouette = silhouette_score(data_to_cluster, labels)\n","    davies_bouldin = davies_bouldin_score(data_to_cluster, labels)\n","\n","    interface_metrics[interface] = {\n","        'best_params': best_params,\n","        'silhouette_score': silhouette,\n","        'davies_bouldin_score': davies_bouldin\n","    }\n","\n","# Compute the mean of the scores across all interfaces\n","total_silhouette = 0\n","total_davies_bouldin = 0\n","num_interfaces = len(interface_metrics)\n","\n","for metrics in interface_metrics.values():\n","    total_silhouette += metrics['silhouette_score']\n","    total_davies_bouldin += metrics['davies_bouldin_score']\n","\n","mean_silhouette = total_silhouette / num_interfaces\n","mean_davies_bouldin = total_davies_bouldin / num_interfaces\n","\n","for interface, metrics in interface_metrics.items():\n","    print(f\"For interface {interface}, best parameters are: {metrics['best_params']}\")\n","    print(f\"Silhouette Score: {metrics['silhouette_score']:.4f}\")\n","    print(f\"Davies-Bouldin Index: {metrics['davies_bouldin_score']:.4f}\")\n","    print('-'*50)\n","\n","# Print the mean scores\n","print(\"Mean Silhouette Score across all interfaces:\", mean_silhouette)\n","print(\"Mean Davies-Bouldin Score across all interfaces:\", mean_davies_bouldin)\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Extracting interface names and their corresponding scores for plotting\n","interface_names = list(interface_metrics.keys())\n","silhouette_scores = [metrics['silhouette_score'] for metrics in interface_metrics.values()]\n","davies_bouldin_scores = [metrics['davies_bouldin_score'] for metrics in interface_metrics.values()]\n","\n","# Sort the interface names and scores based on the size of the column (broadcast_delta)\n","sorted_indices = np.argsort(silhouette_scores)  # You can use silhouette_scores or davies_bouldin_scores\n","interface_names = [interface_names[i] for i in sorted_indices]\n","silhouette_scores = [silhouette_scores[i] for i in sorted_indices]\n","davies_bouldin_scores = [davies_bouldin_scores[i] for i in sorted_indices]\n","\n","# Plotting Silhouette Scores\n","plt.figure(figsize=(15,7))\n","plt.bar(interface_names, silhouette_scores, color='blue')\n","plt.xlabel('Interface')\n","plt.ylabel('Silhouette Score')\n","plt.title('Silhouette Score per Interface')\n","plt.xticks(rotation=90)\n","plt.tight_layout()\n","plt.show()\n","\n","# Plotting Davies-Bouldin Scores\n","plt.figure(figsize=(15,7))\n","plt.bar(interface_names, davies_bouldin_scores, color='red')\n","plt.xlabel('Interface')\n","plt.ylabel('Davies-Bouldin Index')\n","plt.title('Davies-Bouldin Index per Interface')\n","plt.xticks(rotation=90)\n","plt.tight_layout()\n","plt.show()"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["df['kmeans_cluster_label'] = -1  # Initialize to -1\n","\n","# 3. Assign cluster labels to each point using best parameters\n","for interface in interfaces:\n","    interface_data = df[df['switch_interface'] == interface]\n","    data_to_cluster = interface_data['broadcast_delta'].values.reshape(-1, 1)\n","\n","    # Check if the interface exists in the dictionary\n","    if interface in interface_params:\n","        best_params = interface_params[interface]\n","        _, u, _, _, _, _, _ = fuzz.cluster.cmeans(data_to_cluster.T, c=best_params['clusters'], m=best_params['m'], error=0.005, maxiter=1000)\n","\n","        labels = np.argmax(u, axis=0)\n","        labels = labels.flatten()\n","\n","        # Assign cluster labels to the corresponding data points in the original DataFrame\n","        df.loc[df['switch_interface'] == interface, 'kmeans_cluster_label'] = labels\n","\n","# Create a custom color palette to ensure unique colors for each cluster\n","num_clusters = len(df['kmeans_cluster_label'].unique())\n","custom_palette = sns.color_palette(\"Set1\", num_clusters)\n","\n","\n","# Create a larger facet grid with individual plots for each interface\n","g = sns.FacetGrid(df, col=\"switch_interface\", col_wrap=3, height=5, sharey=False)\n","g.map_dataframe(sns.scatterplot, x=\"date\", y=\"broadcast_delta\", hue=\"kmeans_cluster_label\", palette=\"Set1\", alpha=0.5)\n","g.add_legend(title=\"Cluster\")\n","g.set_axis_labels(\"Date\", \"Broadcast Delta\")\n","g.set_titles(\"Interface {col_name}\")\n","\n","# Change legend labels to 'Cluster A', 'Cluster B', 'Cluster C', etc.\n","new_legend_labels = [f\"Cluster {chr(65 + i)}\" for i in range(len(df['kmeans_cluster_label'].unique()))]\n","g._legend.set_title(\"Cluster\")\n","for t, l in zip(g._legend.texts, new_legend_labels):\n","    t.set_text(l)\n","\n","# Show the facet grid plots\n","plt.show()"]},{"cell_type":"markdown","metadata":{},"source":["## ensemble"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Normalize the iForest score to [0,1]\n","df['norm_iforest_score'] = (df['anomaly_iforest_score'] + 1) / 2\n","\n","# Weights\n","weights = {\n","    'norm_iforest_score': 3,\n","    'anomaly_ocsvm': 1,\n","    'anomaly_lof': 1,\n","    'early_ist_anomaly': 0.5\n","}\n","\n","# Calculate weighted ensemble score\n","df['ensemble_score'] = sum(df[col] * weight for col, weight in weights.items())\n","\n","# Thresholding the ensemble score to determine anomalies\n","threshold = 2.5\n","df['ensemble_anomaly'] = (df['ensemble_score'] >= threshold).astype(bool)\n"]},{"cell_type":"markdown","metadata":{},"source":["### fix manually downfall anomalies"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["def remove_downfall_and_low_anomalies(df, threshold=5):\n","    interfaces = df['switch_interface'].unique()\n","    \n","    for interface in interfaces:\n","        subset_df = df[df['switch_interface'] == interface].copy()\n","        \n","        # Iterate over the rows of the subset_df\n","        for idx, row in subset_df.iterrows():\n","            # Ensure that we're not at the boundaries of the dataframe\n","            if idx > 0 and idx < len(subset_df) - 1:\n","                prev_broadcast = df.at[idx - 1, 'broadcast_delta']\n","                next_broadcast = df.at[idx + 1, 'broadcast_delta']\n","                current_broadcast = row['broadcast_delta']\n","                \n","                # Check if the current broadcast is greater than both previous and next\n","                if current_broadcast > prev_broadcast and current_broadcast > next_broadcast:\n","                    continue\n","                \n","                # Check if current broadcast delta is equal to or below the threshold\n","                elif current_broadcast <= threshold:\n","                    for column in ['early_ist_anomaly', 'anomaly_iforest', 'anomaly_ocsvm', 'anomaly_lof', 'ensemble_score']:\n","                        df.at[idx, column] = False\n","                    \n","                else:\n","                    # If the current broadcast value isn't a spike, set anomalies to False\n","                    for column in ['early_ist_anomaly', 'anomaly_iforest', 'anomaly_ocsvm', 'anomaly_lof']:\n","                        df.at[idx, column] = False\n","    \n","    return df\n","\n","# Ensure the specified columns are of type bool\n","df['early_ist_anomaly'] = df['early_ist_anomaly'].astype(bool)\n","df['anomaly_ocsvm'] = df['anomaly_ocsvm'].astype(bool)\n","\n","# Update the dataframe\n","df = remove_downfall_and_low_anomalies(df)\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["interface_thresholds = {\n","    '1:FastEthernet0/10': 25,\n","    '1:FastEthernet0/14': 25,\n","    '1:FastEthernet0/2': 25,\n","    '1:FastEthernet0/22': 50,\n","    '1:FastEthernet0/30': 2,\n","    '1:FastEthernet0/36': 25,\n","    '1:FastEthernet0/37': 20,\n","    '1:FastEthernet0/43': 40,\n","    '1:FastEthernet0/45': 35,\n","    '1:GigabitEthernet0/1': 50,\n","    '1:GigabitEthernet0/2': 0,\n","    '2:GigabitEthernet1/0/36': 5,\n","    '2:GigabitEthernet1/0/37': 5,\n","    '2:GigabitEthernet1/0/38': 0,\n","    '2:GigabitEthernet1/0/39': 0,\n","    '2:GigabitEthernet1/0/40': 0,\n","    '2:TenGigabitEthernet1/1/4': 0\n","}\n","\n","anomaly_columns = ['ensemble_anomaly', 'anomaly_iforest', 'anomaly_ocsvm', 'anomaly_lof', 'early_ist_anomaly']\n","\n","for interface, threshold in interface_thresholds.items():\n","    mask = (df['switch_interface'] == interface) & (df['broadcast_delta'] < threshold)\n","    for col in anomaly_columns:\n","        df.loc[mask, col] = False\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["\n","for interface in unique_interfaces:\n","    subset_df = df[df['switch_interface'] == interface].copy()\n","    \n","    # Create traces for broadcast delta\n","    trace0 = go.Scatter(\n","        x = subset_df['date'],\n","        y = subset_df['broadcast_delta'],\n","        mode = 'lines',\n","        name = 'Broadcast Delta'\n","    )\n","    \n","    # Create traces for anomalies detected by Ensemble\n","    trace1 = go.Scatter(\n","        x = subset_df[subset_df['anomaly_iforest']]['date'],\n","        y = subset_df[subset_df['anomaly_iforest']]['broadcast_delta'],\n","        mode = 'markers',\n","        name = 'Anomalies by iforest',\n","        marker=dict(color='red', size=8)\n","    )\n","    \n","    layout = go.Layout(\n","        title=f'Broadcast Delta with Anomalies by iforest for Interface {interface}',\n","        xaxis=dict(title='Date'),\n","        yaxis=dict(title='Broadcast Delta'),\n","    )\n","    \n","    fig = go.Figure(data=[trace0, trace1], layout=layout)\n","    fig.show()\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Function to plot density for normal data\n","def plot_kde_normal(data, color, label):\n","    normal_data = data[data['anomaly_iforest'] == False]['broadcast_delta']\n","    sns.kdeplot(data=normal_data, color=color, label=label)\n","\n","# Function to plot density for anomalies\n","def plot_kde_anomaly(data, color, label):\n","    anomaly_data = data[data['anomaly_iforest'] == True]['broadcast_delta']\n","    sns.kdeplot(data=anomaly_data, color=color, label=label)\n","\n","# Create FacetGrid\n","g = sns.FacetGrid(df, col=\"switch_interface\", col_wrap=4, height=4, sharex=False,sharey=False)\n","g.map_dataframe(plot_kde_normal, color='b', label='Normal Data')\n","g.map_dataframe(plot_kde_anomaly, color='r', label='Anomaly').add_legend()\n","\n","plt.show()\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Subset of dataframe with just the anomaly tags\n","anomaly_tags = df[['anomaly_iforest', 'anomaly_ocsvm', 'anomaly_lof', 'early_ist_anomaly']]\n","\n","# Compute correlation matrix\n","correlation_matrix = anomaly_tags.corr()\n","\n","# Update column names for plotting\n","correlation_matrix.columns = ['iforest', 'ocsvm', 'lof', 'ensemble']\n","correlation_matrix.index = ['iforest', 'ocsvm', 'lof', 'ensemble']\n","\n","plt.figure(figsize=(10, 8))\n","sns.heatmap(correlation_matrix, annot=True, cmap='RdBu_r', center=0)\n","plt.title('Correlation Heatmap of Anomaly Taggings')\n","plt.show()"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Assuming df has a 'switch_interface' column and 'anomaly_iforest' is your iForest binary output (1 for anomalies, 0 for non-anomalies)\n","interfaces = df['switch_interface'].unique()\n","\n","silhouette_scores = []\n","db_scores = []\n","\n","for interface in interfaces:\n","    subset = df[df['switch_interface'] == interface]\n","    if len(subset['anomaly_iforest'].unique()) > 1:  # Need at least 2 clusters to compute these scores\n","        sil_score = silhouette_score(subset[['broadcast_delta']], subset['anomaly_iforest'])\n","        db_score = davies_bouldin_score(subset[['broadcast_delta']], subset['anomaly_iforest'])\n","        silhouette_scores.append(sil_score)\n","        db_scores.append(db_score)\n","    else:\n","        silhouette_scores.append(None)  # or some placeholder value\n","        db_scores.append(None)\n","\n","# Plotting\n","plt.figure(figsize=(14, 5))\n","plt.subplot(1, 2, 1)\n","plt.bar(interfaces, silhouette_scores, color='blue')\n","plt.title('Silhouette Scores per Interface')\n","plt.xticks(rotation=90)\n","\n","plt.subplot(1, 2, 2)\n","plt.bar(interfaces, db_scores, color='red')\n","plt.title('Davies-Bouldin Scores per Interface')\n","plt.xticks(rotation=90)\n","\n","plt.tight_layout()\n","plt.show()\n"]},{"cell_type":"markdown","metadata":{},"source":["A high silhouette score could imply that anomalies are well-separated from the normal data. However, it's important to note that not all anomaly detection problems will have clear separations, especially with contextual anomalies."]},{"cell_type":"markdown","metadata":{},"source":["A low Davies-Bouldin Index could suggest that the anomalies form a tight cluster separate from the normal data"]}],"metadata":{"deepnote":{},"deepnote_execution_queue":[],"deepnote_notebook_id":"9ad0f57f888b41db86a0cd8d058c144e","kernelspec":{"display_name":"base","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.8"},"orig_nbformat":2},"nbformat":4,"nbformat_minor":0}
